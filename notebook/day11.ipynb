{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Day 11: 使用 Weights & Biases (WandB) 进行实验追踪\n",
                "\n",
                "在深度学习实验中，记录和可视化训练过程（Loss, Accuracy, 显存占用等）至关重要。**Weights & Biases (WandB)** 是目前最流行的实验追踪工具之一，它比 TensorBoard 更强大，支持云端存储、多人协作、超参搜索等功能。\n",
                "\n",
                "**学习目标：**\n",
                "- 掌握 WandB 的基本配置与初始化 (`wandb.init`)\n",
                "- 学会记录指标 (`wandb.log`)\n",
                "- 将 WandB 集成到 PyTorch 训练循环中\n",
                "- 将 WandB 集成到 Hugging Face Trainer 中\n",
                "\n",
                "**前置准备：**\n",
                "1. 注册 WandB 账号: [https://wandb.ai/](https://wandb.ai/)\n",
                "2. 获取 API Key: [https://wandb.ai/authorize](https://wandb.ai/authorize)\n",
                "3. 安装库: `pip install wandb`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import wandb\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torchvision import datasets, transforms\n",
                "from torch.utils.data import DataLoader\n",
                "import random\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. 登录 WandB\n",
                "\n",
                "在 Notebook 中运行以下代码进行登录。如果是第一次运行，会提示输入 API Key。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wandb.login()\n",
                "# 如果已经登录过，会自动读取本地配置"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. WandB 基础概念\n",
                "\n",
                "- **Project**: 项目，对应一个具体的任务（例如 \"cifar10-classification\"）。\n",
                "- **Run**: 运行，对应一次具体的实验（例如 \"resnet18-lr0.01\"）。\n",
                "- **Config**: 超参数配置，记录本次实验的设置（LR, Batch Size, Optimizer 等）。\n",
                "- **Log**: 记录随时间变化的指标（Loss, Accuracy）。"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. 集成到 PyTorch 训练循环\n",
                "\n",
                "我们将使用一个简单的 MLP 在 MNIST 上演示 WandB 的集成。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. 定义超参数配置\n",
                "config = {\n",
                "    \"learning_rate\": 0.01,\n",
                "    \"epochs\": 5,\n",
                "    \"batch_size\": 64,\n",
                "    \"architecture\": \"MLP\",\n",
                "    \"dataset\": \"MNIST\"\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_model():\n",
                "    # 2. 初始化 WandB Run\n",
                "    # with wandb.init(...) 可以在代码块结束时自动 finish，非常推荐\n",
                "    with wandb.init(project=\"handonDL-day11-demo\", config=config):\n",
                "        \n",
                "        # 通过 wandb.config 访问超参数（这样如果使用 sweep 超参搜索，wandb 会自动注入参数）\n",
                "        cfg = wandb.config\n",
                "        \n",
                "        # 数据准备\n",
                "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
                "        train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
                "        train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
                "        \n",
                "        # 模型定义\n",
                "        model = nn.Sequential(\n",
                "            nn.Flatten(),\n",
                "            nn.Linear(28*28, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, 10)\n",
                "        )\n",
                "        \n",
                "        optimizer = optim.SGD(model.parameters(), lr=cfg.learning_rate)\n",
                "        criterion = nn.CrossEntropyLoss()\n",
                "        \n",
                "        # 3. 监控模型梯度和参数（可选）\n",
                "        wandb.watch(model, log=\"all\", log_freq=100)\n",
                "        \n",
                "        print(\"Start Training...\")\n",
                "        for epoch in range(cfg.epochs):\n",
                "            model.train()\n",
                "            for batch_idx, (data, target) in enumerate(train_loader):\n",
                "                optimizer.zero_grad()\n",
                "                output = model(data)\n",
                "                loss = criterion(output, target)\n",
                "                loss.backward()\n",
                "                optimizer.step()\n",
                "                \n",
                "                if batch_idx % 100 == 0:\n",
                "                    # 4. 记录指标\n",
                "                    wandb.log({\"epoch\": epoch, \"loss\": loss.item()})\n",
                "            \n",
                "            # 模拟验证集准确率\n",
                "            val_acc = 0.9 + 0.01 * epoch + random.uniform(-0.005, 0.005)\n",
                "            wandb.log({\"val_acc\": val_acc})\n",
                "            print(f\"Epoch {epoch}: Val Acc {val_acc:.4f}\")\n",
                "            \n",
                "    # with 语句结束时自动调用 wandb.finish()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 运行训练\n",
                "train_model()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. 集成到 Hugging Face Trainer\n",
                "\n",
                "Hugging Face 的 `Trainer` 原生支持 WandB，集成非常简单，只需要在 `TrainingArguments` 中设置 `report_to=\"wandb\"`。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoTokenizer\n",
                "\n",
                "# 示例代码（不实际运行，因为需要下载大模型）\n",
                "def hf_integration_demo():\n",
                "    training_args = TrainingArguments(\n",
                "        output_dir=\"./results\",\n",
                "        num_train_epochs=3,\n",
                "        per_device_train_batch_size=8,\n",
                "        logging_steps=10,\n",
                "        \n",
                "        # 关键参数\n",
                "        report_to=\"wandb\",  # 启用 WandB 报告\n",
                "        run_name=\"bert-finetune-demo\" # 设置 WandB Run 的名称\n",
                "    )\n",
                "    \n",
                "    # model = ...\n",
                "    # trainer = Trainer(\n",
                "    #     model=model,\n",
                "    #     args=training_args,\n",
                "    #     ...\n",
                "    # )\n",
                "    # trainer.train()\n",
                "    \n",
                "    print(\"Hugging Face Trainer 集成示例：设置 report_to='wandb' 即可。\")\n",
                "\n",
                "hf_integration_demo()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. 进阶功能\n",
                "\n",
                "- **Artifacts**: 用于版本化管理数据集和模型文件。`run.log_artifact(artifact_obj)`\n",
                "- **Sweeps**: 自动化超参数搜索。WandB 提供了一个强大的 Sweep Controller，可以自动尝试不同的超参组合。\n",
                "- **Tables**: 记录表格数据（如文本生成结果、图片预测结果），支持在 Web 界面进行交互式分析。\n",
                "\n",
                "## 6. 总结\n",
                "\n",
                "WandB 是现代深度学习工作流中不可或缺的工具。相比于本地的日志文件，它提供了：\n",
                "1. **实时可视化**：无需等待训练结束。\n",
                "2. **集中管理**：所有实验记录在云端，不再丢失。\n",
                "3. **对比分析**：轻松对比不同 Run 之间的效果。\n",
                "\n",
                "建议在日后的所有实验中都开启 WandB。"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}