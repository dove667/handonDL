{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Day 10: 分布式训练与推理 (Distributed Training & Inference)\n",
                "\n",
                "随着模型规模和数据量的增长，单张 GPU 往往无法满足训练需求。本章将介绍 PyTorch 中的并行训练机制，重点讲解 **DataParallel (DP)** 和 **DistributedDataParallel (DDP)** 的区别与使用。\n",
                "\n",
                "**学习目标：**\n",
                "- 理解模型并行 (Model Parallelism) 与数据并行 (Data Parallelism) 的区别\n",
                "- 掌握 `nn.DataParallel` 的使用（简单，适用于单机多卡）\n",
                "- 理解 `DistributedDataParallel` (DDP) 的核心概念（高效，适用于多机多卡）\n",
                "- 了解 Hugging Face `Accelerate` 库如何简化分布式训练\n",
                "\n",
                "**注意：**\n",
                "由于 Jupyter Notebook 的交互式特性，直接演示多进程的 DDP 比较困难（通常需要通过 `torchrun` 命令行启动）。本 Notebook 主要演示 `DataParallel`，并提供 DDP 的代码模板供脚本运行使用。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "from torchvision import datasets, transforms\n",
                "import time"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. 检查 GPU 数量\n",
                "\n",
                "首先检查当前环境可用的 GPU 数量。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "可用 GPU 数量: 8\n",
                        "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
                        "GPU 1: NVIDIA GeForce RTX 2080 Ti\n",
                        "GPU 2: NVIDIA GeForce RTX 2080 Ti\n",
                        "GPU 3: NVIDIA GeForce RTX 2080 Ti\n",
                        "GPU 4: NVIDIA GeForce RTX 2080 Ti\n",
                        "GPU 5: NVIDIA GeForce RTX 2080 Ti\n",
                        "GPU 6: NVIDIA GeForce RTX 2080 Ti\n",
                        "GPU 7: NVIDIA GeForce RTX 2080 Ti\n"
                    ]
                }
            ],
            "source": [
                "if torch.cuda.is_available():\n",
                "    device_count = torch.cuda.device_count()\n",
                "    print(f\"可用 GPU 数量: {device_count}\")\n",
                "    for i in range(device_count):\n",
                "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
                "else:\n",
                "    print(\"当前环境无 GPU，将使用 CPU 演示（无法体现并行加速）。\")\n",
                "    device_count = 0\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. DataParallel (DP)\n",
                "\n",
                "`nn.DataParallel` 是 PyTorch 中最简单的并行方式。它通过在 batch 维度上将输入数据切分到多个 GPU 上，在每个 GPU 上复制模型副本进行前向计算，然后在主 GPU 上收集输出并计算损失，最后将梯度同步回各个 GPU。\n",
                "\n",
                "**优点：**\n",
                "- 代码修改极少，只需一行代码封装模型。\n",
                "\n",
                "**缺点：**\n",
                "- **单进程多线程**：受 Python GIL 限制，扩展性差。\n",
                "- **负载不均**：主 GPU (GPU 0) 负责收集输出和计算 Loss，显存占用和计算压力通常比其他 GPU 大。\n",
                "- 仅支持单机多卡。\n",
                "\n",
                "### 2.1 定义一个简单的模型"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleModel(nn.Module):\n",
                "    def __init__(self):\n",
                "        super(SimpleModel, self).__init__()\n",
                "        self.fc1 = nn.Linear(32 * 32 * 3, 512)\n",
                "        self.fc2 = nn.Linear(512, 10)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = x.view(x.size(0), -1)\n",
                "        x = self.fc1(x)\n",
                "        x = torch.relu(x)\n",
                "        x = self.fc2(x)\n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 使用 DataParallel 封装模型\n",
                "\n",
                "核心代码：`model = nn.DataParallel(model)`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "使用 8 个 GPU 进行训练！\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "DataParallel(\n",
                            "  (module): SimpleModel(\n",
                            "    (fc1): Linear(in_features=3072, out_features=512, bias=True)\n",
                            "    (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
                            "  )\n",
                            ")"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model = SimpleModel()\n",
                "\n",
                "if device_count > 1:\n",
                "    print(f\"使用 {device_count} 个 GPU 进行训练！\")\n",
                "    # 这一行是关键\n",
                "    model = nn.DataParallel(model)\n",
                "\n",
                "model.to(device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3 模拟训练过程\n",
                "\n",
                "训练循环本身几乎不需要修改。注意 `inputs` 和 `labels` 需要移动到 `device`（通常是 GPU 0），`DataParallel` 会自动将数据切分分发到其他 GPU。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "开始模拟训练步骤...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/cseadmin/zsh/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)\n",
                        "  return F.linear(input, self.weight, self.bias)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Step 1, Loss: 2.3217\n",
                        "Step 2, Loss: 2.2402\n",
                        "Step 3, Loss: 2.1612\n",
                        "Step 4, Loss: 2.0847\n",
                        "Step 5, Loss: 2.0105\n",
                        "Step 6, Loss: 1.9387\n",
                        "Step 7, Loss: 1.8690\n",
                        "Step 8, Loss: 1.8014\n",
                        "Step 9, Loss: 1.7359\n",
                        "Step 10, Loss: 1.6724\n",
                        "训练完成，耗时: 3.1450s\n"
                    ]
                }
            ],
            "source": [
                "# 准备虚拟数据\n",
                "inputs = torch.randn(64, 3, 32, 32)\n",
                "labels = torch.randint(0, 10, (64,))\n",
                "\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
                "\n",
                "print(\"开始模拟训练步骤...\")\n",
                "start_time = time.time()\n",
                "\n",
                "for i in range(10):\n",
                "    inputs, labels = inputs.to(device), labels.to(device)\n",
                "    \n",
                "    optimizer.zero_grad()\n",
                "    outputs = model(inputs)\n",
                "    loss = criterion(outputs, labels)\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    \n",
                "    print(f\"Step {i+1}, Loss: {loss.item():.4f}\")\n",
                "\n",
                "print(f\"训练完成，耗时: {time.time() - start_time:.4f}s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. DistributedDataParallel (DDP)\n",
                "\n",
                "`nn.parallel.DistributedDataParallel` 是 PyTorch 推荐的并行方式，即使是单机多卡也推荐使用。\n",
                "\n",
                "**原理：**\n",
                "- **多进程**：每个 GPU 对应一个独立的进程，避免了 GIL 限制。\n",
                "- **梯度同步**：每个进程拥有完整的模型副本，只处理部分数据。在反向传播时，通过 `Ring AllReduce` 算法高效同步梯度。\n",
                "\n",
                "**核心概念：**\n",
                "- **Rank**：进程的全局 ID（例如 4 张卡，Rank 为 0-3）。\n",
                "- **World Size**：总进程数（总 GPU 数）。\n",
                "- **Process Group**：进程组，用于通信。\n",
                "- **DistributedSampler**：确保每个进程读取不同的数据切片。\n",
                "\n",
                "### 3.1 DDP 代码结构示例 (伪代码)\n",
                "\n",
                "DDP 通常需要通过脚本运行，以下是标准结构：\n",
                "\n",
                "```python\n",
                "import torch.distributed as dist\n",
                "from torch.nn.parallel import DistributedDataParallel as DDP\n",
                "from torch.utils.data.distributed import DistributedSampler\n",
                "\n",
                "def setup(rank, world_size):\n",
                "    # 初始化进程组\n",
                "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
                "\n",
                "def cleanup():\n",
                "    dist.destroy_process_group()\n",
                "\n",
                "def main(rank, world_size):\n",
                "    setup(rank, world_size)\n",
                "    \n",
                "    # 1. 模型移动到对应 GPU\n",
                "    model = SimpleModel().to(rank)\n",
                "    # 2. 封装 DDP\n",
                "    model = DDP(model, device_ids=[rank])\n",
                "    \n",
                "    # 3. 数据采样器\n",
                "    dataset = ...\n",
                "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
                "    dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
                "    \n",
                "    # 4. 训练循环\n",
                "    for epoch in range(epochs):\n",
                "        sampler.set_epoch(epoch) # 重要：打乱数据\n",
                "        for data, target in dataloader:\n",
                "            data, target = data.to(rank), target.to(rank)\n",
                "            ...\n",
                "            \n",
                "    cleanup()\n",
                "\n",
                "# 启动方式：\n",
                "# torchrun --nproc_per_node=4 script.py\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Hugging Face Accelerate\n",
                "\n",
                "手动编写 DDP 代码比较繁琐（涉及 setup, cleanup, rank 管理等）。Hugging Face 推出的 `Accelerate` 库极大地简化了这一过程，让你用同一套代码在 CPU、单 GPU、多 GPU (DDP)、TPU 上运行。\n",
                "\n",
                "### 4.1 使用 Accelerate 改造代码\n",
                "\n",
                "```python\n",
                "from accelerate import Accelerator\n",
                "\n",
                "accelerator = Accelerator()\n",
                "\n",
                "model = SimpleModel()\n",
                "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
                "dataloader = DataLoader(...)\n",
                "\n",
                "# 自动准备所有对象\n",
                "model, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader)\n",
                "\n",
                "for data, target in dataloader:\n",
                "    optimizer.zero_grad()\n",
                "    output = model(data)\n",
                "    loss = criterion(output, target)\n",
                "    \n",
                "    # 替代 loss.backward()\n",
                "    accelerator.backward(loss)\n",
                "    \n",
                "    optimizer.step()\n",
                "```\n",
                "\n",
                "**启动方式：**\n",
                "```bash\n",
                "accelerate config  # 配置环境（只需一次）\n",
                "accelerate launch script.py # 启动脚本\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. 总结\n",
                "\n",
                "| 特性 | DataParallel (DP) | DistributedDataParallel (DDP) |\n",
                "| :--- | :--- | :--- |\n",
                "| **适用场景** | 单机多卡，快速验证 | 单机多卡，多机多卡，生产环境 |\n",
                "| **实现原理** | 单进程多线程，参数服务器模式 | 多进程，Ring AllReduce |\n",
                "| **性能** | 较差（GIL 锁，GPU 0 负载高） | 优（无 GIL，负载均衡） |\n",
                "| **易用性** | 极简（一行代码） | 较复杂（需处理进程组、采样器） |\n",
                "\n",
                "**建议：**\n",
                "- 简单的实验或 Debug：使用 `DataParallel`。\n",
                "- 正式训练或大规模数据：使用 `DDP` 或 `Accelerate`。"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv (3.10.12)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
