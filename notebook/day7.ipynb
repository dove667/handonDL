{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e03ab8",
   "metadata": {},
   "source": [
    "# Day 7: Hugging Face Transformers 基础（Pipeline、Tokenizer、Model）\n",
    "\n",
    "本讲义介绍 Hugging Face `transformers` 库的三个核心组件，这是使用预训练 Transformer 模型的基础。\n",
    "\n",
    "**学习目标：**\n",
    "- 理解 Pipeline 的完整工作流（预处理 → 推理 → 后处理）\n",
    "- 掌握 Tokenizer 的使用：分词、填充、截断\n",
    "- 学习 AutoModel 的加载与前向传播\n",
    "- 认识不同任务对应的 AutoModel 变体\n",
    "\n",
    "**三个关键抽象：**\n",
    "1. **Pipeline**：高层接口，一行代码解决任务（快速原型）\n",
    "2. **Tokenizer**：文本处理器，将原始文本转换为模型输入\n",
    "3. **Model**：神经网络，进行实际计算\n",
    "\n",
    "**提示**：Pipeline 方便快速测试，但微调时通常需要直接使用 Tokenizer 和 Model。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4082023",
   "metadata": {},
   "source": [
    "## Pipeline：一行代码完成情感分析\n",
    "\n",
    "**Pipeline** 是 Hugging Face 提供的最高层抽象，封装了整个推理流程：\n",
    "\n",
    "**工作流程：**\n",
    "```\n",
    "用户输入文本 \n",
    "    ↓\n",
    "Tokenizer 分词 & 数值化\n",
    "    ↓\n",
    "Model 推理（获得原始输出 logits）\n",
    "    ↓\n",
    "后处理（softmax + 格式化）\n",
    "    ↓\n",
    "返回人类可读结果 {label, score}\n",
    "```\n",
    "\n",
    "**优点**：\n",
    "- 零配置，开箱即用\n",
    "- 自动选择合适的模型架构\n",
    "- 支持批量输入\n",
    "\n",
    "**缺点**：\n",
    "- 功能有限（无法自定义损失、优化器等）\n",
    "- 推理速度相对较慢（因为自动化带来的开销）\n",
    "- 不适合微调任务\n",
    "\n",
    "**使用场景**：快速原型、Demo、教学"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66acec25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f14efdd77d419792f64e940c85a603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4103d715bdb484bb7972866689cf251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b07743642748b38768e61a4c200773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7286ce62a6d4d78a0c7a05325061f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7047ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier #防止占用内存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d581e97",
   "metadata": {},
   "source": [
    "这里把整个模型都加载进了内存，进行推理。（67M，F32）       \n",
    "查看模型相信信息。https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a240eedb",
   "metadata": {},
   "source": [
    "Pipeline的工作流程包含以下步骤：\n",
    "\n",
    "根据输入的任务类型和 checkpoint，自动加载对应的 tokenizer 和 model\n",
    "\n",
    "- 预处理 (Preprocessing):- 将输入文本转换为模型可以理解的格式- 进行分词 (tokenization)- 添加特殊标记（如[CLS], [SEP]等）- 将token转换为对应的ID\n",
    "- 模型推理 (Model Inference):- 将处理后的输入传入预训练模型- 模型进行计算并输出原始预测结果\n",
    "- 后处理 (Post-processing):- 将模型的原始输出转换为人类可理解的格式- 对结果进行格式化（如标签和置信度分数）\n",
    "\n",
    "输入：\n",
    "\n",
    "- 可以是单个文本字符串或文本列表\n",
    "- 支持不同任务类型的特定输入格式\n",
    "\n",
    "输出：\n",
    "\n",
    "- 返回包含预测结果的字典或字典列表\n",
    "- 结果通常包含：- label: 预测的标签- score: 预测的置信度分数（0-1之间）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5666a825",
   "metadata": {},
   "source": [
    "## Tokenizer：将文本转换为数字\n",
    "\n",
    "**Tokenizer** 的核心责任：\n",
    "1. **分词（Tokenization）**：将文本拆分为 token（通常是子词）\n",
    "2. **映射（Mapping）**：将 token 转换为对应的 ID\n",
    "3. **填充与截断**：统一序列长度\n",
    "4. **特殊标记**：添加 `[CLS]`, `[SEP]` 等模型特定的标记\n",
    "\n",
    "**输出格式（字典）：**\n",
    "- `input_ids`：token ID 序列\n",
    "- `attention_mask`：1 表示有效 token，0 表示填充\n",
    "- `token_type_ids`：用于句子对任务，区分两个句子\n",
    "\n",
    "**关键参数：**\n",
    "- `max_length`：截断或填充到指定长度\n",
    "- `padding`：填充方式（`\"max_length\"`、`\"longest\"`）\n",
    "- `truncation`：是否截断超长序列\n",
    "- `return_tensors`：返回 PyTorch 张量（`\"pt\"`）还是 NumPy（`\"np\"`）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6ac546",
   "metadata": {},
   "source": [
    "## AutoClass 的智能推断\n",
    "\n",
    "**AutoClass（AutoTokenizer、AutoModel 等）如何工作？**\n",
    "\n",
    "1. **根据 checkpoint 查询配置**：\n",
    "   - 从 Hub 或本地路径下载 `config.json`\n",
    "   - 读取 `model_type` 字段（如 `\"bert\"`、`\"roberta\"`）\n",
    "\n",
    "2. **自动选择正确的类**：\n",
    "   - `model_type=\"bert\"` → `BertTokenizer` 或 `BertModel`\n",
    "   - 无需手动指定类名，减少出错\n",
    "\n",
    "3. **加载预训练权重**：\n",
    "   - 自动下载并缓存模型权重\n",
    "   - 一致性检查确保配置与权重匹配\n",
    "\n",
    "**注意**：AutoClass 进行**精确匹配**，不做模糊匹配。\n",
    "- ✅ `AutoModel.from_pretrained(\"bert-base-uncased\")` → 成功\n",
    "- ❌ `AutoModel.from_pretrained(\"ber-base-uncased\")` → 错误（拼写错误）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c69b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a72632451464adc89fd02a05b0f32df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172798f39b6d485aa798140ea3cc7929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb7ad2b240649fd8639257500efceca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36972661dda7466e9e460dda7240d7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本: Hello, how are you today? I am Grok, built by xAI!\n",
      "\n",
      "Tokenizer 输出:\n",
      "input_ids: tensor([[  101,  7592,  1010,  2129,  2024,  2017,  2651,  1029,  1045,  2572,\n",
      "         24665,  6559,  1010,  2328,  2011,  1060,  4886,   999,   102,     0]])\n",
      "attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])\n",
      "token_type_ids: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "解码回文本: [CLS] hello, how are you today? i am grok, built by xai! [SEP] [PAD]\n",
      "分词结果: ['[CLS]', 'hello', ',', 'how', 'are', 'you', 'today', '?', 'i', 'am', 'gr', '##ok', ',', 'built', 'by', 'x', '##ai', '!', '[SEP]', '[PAD]']\n",
      "\n",
      "批量输入结果:\n",
      "input_ids: tensor([[  101,  7592,  2088,   999,   102,     0,     0],\n",
      "        [  101,  1045,  2572, 24665,  6559,  1012,   102]])\n",
      "attention_mask: tensor([[1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "# 1. 加载 AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"bert-base-uncased\",        # checkpoint 名称\n",
    "        use_fast=True,              # 使用快速分词器（推荐，基于 Rust 实现）\n",
    "        add_prefix_space=False,     # 是否在开头添加空格（对某些模型如 RoBERTa 有用）\n",
    ")\n",
    "    \n",
    "# 2. 输入文本\n",
    "text = \"Hello, how are you today? I am Grok, built by xAI!\"\n",
    "    \n",
    "# 3. 使用 tokenizer 处理文本（设置常用参数）\n",
    "encoded_output = tokenizer(\n",
    "    text,                       # 输入文本（字符串或字符串列表）\n",
    "    add_special_tokens=True,    # 是否添加特殊标记（如 [CLS], [SEP]）\n",
    "    max_length=20,             # 最大序列长度（超过会截断）\n",
    "    padding=\"max_length\",       # 填充到 max_length（可选：\"longest\", False 等）\n",
    "    truncation=True,            # 超过 max_length 时截断\n",
    "    return_tensors=\"pt\",        # 返回 PyTorch 张量（可选：\"tf\" 或 None）\n",
    "    return_attention_mask=True, # 返回 attention mask\n",
    "    return_token_type_ids=True, # 返回 token type IDs（用于区分句子对。在句子对任务中，第一个句子为 0，第二个句子为 1）\n",
    ")\n",
    "    \n",
    "# 4. 输出结果 字典\n",
    "print(\"原始文本:\", text)\n",
    "print(\"\\nTokenizer 输出:\")\n",
    "print(\"input_ids:\", encoded_output[\"input_ids\"])\n",
    "print(\"attention_mask:\", encoded_output[\"attention_mask\"])\n",
    "print(\"token_type_ids:\", encoded_output[\"token_type_ids\"])\n",
    "print(\"\\n解码回文本:\", tokenizer.decode(encoded_output[\"input_ids\"][0]))\n",
    "print(\"分词结果:\", tokenizer.convert_ids_to_tokens(encoded_output[\"input_ids\"][0]))\n",
    "    \n",
    "# 5. 额外功能：批量输入\n",
    "batch_text = [\"Hello world!\", \"I am Grok.\"]\n",
    "batch_encoded = tokenizer(\n",
    "    batch_text,\n",
    "    padding=True,               # 自动填充到最长序列长度\n",
    "    truncation=True,\n",
    "    max_length=10,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(\"\\n批量输入结果:\")\n",
    "print(\"input_ids:\", batch_encoded[\"input_ids\"])\n",
    "print(\"attention_mask:\", batch_encoded[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0858b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830729cc",
   "metadata": {},
   "source": [
    "**输入参数**\n",
    "    \n",
    "1. **from_pretrained() 参数**：\n",
    "    - \"bert-base-uncased\": 指定预训练模型/checkpoint。\n",
    "    - use_fast=True: 使用 Rust 实现的快速分词器，性能更好。\n",
    "    - cache_dir: 指定缓存路径，避免重复下载。\n",
    "    - add_prefix_space: 对某些模型（如 RoBERTa）有用，BERT 不需要。\n",
    "2. **tokenizer() 参数**：\n",
    "    - text: 输入可以是单个字符串或字符串列表。\n",
    "    - add_special_tokens: 添加模型特定的标记（如 [CLS] 和 [SEP]）。\n",
    "    - max_length: 限制序列长度。\n",
    "    - padding: 填充方式（\"max_length\" 填充到指定长度，\"longest\" 填充到批次中最长序列）。\n",
    "    - truncation: 超过 max_length 时截断。\n",
    "    - return_tensors: 指定返回类型（\"pt\" 为 PyTorch，\"tf\" 为 TensorFlow，None 为 Python 列表）。\n",
    "    - return_attention_mask: 返回注意力掩码，用于区分有效 token 和填充。\n",
    "    - return_token_type_ids: 返回 token 类型 ID，用于句子对任务。\n",
    "    \n",
    "**输出内容**\n",
    "    \n",
    "1. **input_ids**：\n",
    "    - 将文本转换为 token ID 的序列，每个 ID 对应词汇表中的一个 token。\n",
    "    - [CLS]（101）和 [SEP]（102）是特殊标记，[PAD]（0）是填充。\n",
    "2. **attention_mask**：\n",
    "    - 二进制掩码，1 表示有效 token，0 表示填充。\n",
    "    - 用于告诉模型哪些部分需要关注。\n",
    "3. **token_type_ids**：\n",
    "    - 用于区分不同句子（在单句输入中通常全为 0）。\n",
    "    - 在句子对任务中，第一个句子为 0，第二个句子为 1。\n",
    "4. **解码和分词**：\n",
    "    - decode(): 将 input_ids 转换回可读文本。\n",
    "    - convert_ids_to_tokens(): 显示具体的分词结果（如 \"x\" 和 \"##ai\" 是子词）。\n",
    "    \n",
    "**批量输入**\n",
    "    \n",
    "- 当输入是列表时，padding=True 会自动对齐序列长度，填充较短的句子。\n",
    "- max_length 限制仍然有效。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704a1ff7",
   "metadata": {},
   "source": [
    "**扩展功能**\n",
    "    \n",
    "1. **保存 tokenizer**：python\n",
    "        \n",
    "    ```python\n",
    "    tokenizer.save_pretrained(\"./my_tokenizer\")\n",
    "    ```\n",
    "        \n",
    "2. **处理句子对**：python\n",
    "        \n",
    "    ```python\n",
    "    encoded_pair = tokenizer(\"Hello!\", \"How are you?\", return_tensors=\"pt\")\n",
    "    print(encoded_pair[\"token_type_ids\"])  # 区分两个句子\n",
    "    ```\n",
    "        \n",
    "3. **自定义词汇表**：python\n",
    "        \n",
    "    ```python\n",
    "    tokenizer.add_tokens([\"new_token\"])\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f40cc4e",
   "metadata": {},
   "source": [
    "## Model：获取预训练特征表示\n",
    "\n",
    "**AutoModel 与任务特定模型的区别：**\n",
    "\n",
    "| 类 | 输出 | 用途 |\n",
    "|------|------|------|\n",
    "| `AutoModel` | `last_hidden_state`, `pooler_output` | 特征提取、迁移学习 |\n",
    "| `AutoModelForSequenceClassification` | `logits` [batch, num_labels] | 文本分类 |\n",
    "| `AutoModelForTokenClassification` | `logits` [batch, seq_len, num_labels] | NER、词性标注 |\n",
    "| `AutoModelForQuestionAnswering` | `start_logits`, `end_logits` | 抽取式问答 |\n",
    "| `AutoModelForCausalLM` | `logits` [batch, seq_len, vocab_size] | 文本生成（GPT 类） |\n",
    "\n",
    "**重要参数：**\n",
    "- `output_hidden_states=True`：返回所有层的隐藏状态（用于分析）\n",
    "- `output_attentions=True`：返回注意力权重（用于可解释性分析）\n",
    "\n",
    "**性能提示**：启用上述参数会**显著增加内存占用**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b133af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abafd077c14413e8ac2d62609ebeb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66e53e6bc934aec958cfd3d9385e858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "599815b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f38f24",
   "metadata": {},
   "source": [
    "1. **加载模型**：\n",
    "    - 根据 checkpoint 的配置文件，自动推断模型类型（如 BertModel）。\n",
    "    - 使用 AutoModel.from_pretrained(checkpoint) 加载预训练模型。\n",
    "2. **输入数据**：\n",
    "    - AutoModel 需要接收 input_ids（来自 tokenizer 的输出）以及其他可选输入（如 attention_mask）。\n",
    "    - 输入通常是张量（PyTorch 或 TensorFlow）。\n",
    "3. **输出**：\n",
    "    - 模型返回一个包含隐藏状态（hidden states）和其他输出的对象（具体取决于模型类型和配置）。\n",
    "    - 不同任务的 AutoModel 变体返回值不同，需根据任务选择合适的类。\n",
    "    - 默认 AutoModel 不包含头部（head），仅返回隐藏状态。\n",
    "\n",
    "| **变体** | **主要输出字段** | **输出形状示例（BERT）** | **典型任务** |\n",
    "| --- | --- | --- | --- |\n",
    "| AutoModel | last_hidden_state, pooler_output | [1, 10, 768], [1, 768] | 特征提取 |\n",
    "| AutoModelForSequenceClassification | logits | [1, 2] | 文本分类 |\n",
    "| AutoModelForTokenClassification | logits | [1, 10, 3] | NER、词性标注 |\n",
    "| AutoModelForQuestionAnswering | start_logits, end_logits | [1, 10], [1, 10] | 问答 |\n",
    "| AutoModelForCausalLM | logits | [1, 10, 30522] | 文本生成（非 BERT） |\n",
    "| AutoModelForMaskedLM | logits | [1, 10, 30522] | 掩码预测 |\n",
    "| AutoModelForSeq2SeqLM | logits, encoder_last_hidden_state | [1, 10, vocab_size], [1, 10, 768] | 翻译、摘要（非 BERT） |\n",
    "\n",
    "返回logit的话到分类还要经过\n",
    "\n",
    "`torch.nn.functional.softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880534ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb14639cc004048b2c191d02bc04f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本: Hello, how are you today? I am Grok, built by xAI!\n",
      "\n",
      "Model 输出:\n",
      "最后一层隐藏状态 (last_hidden_state): torch.Size([1, 20, 768])\n",
      "池化输出 (pooler_output): torch.Size([1, 768])\n",
      "所有隐藏状态数量: 13\n",
      "注意力权重数量: 12\n",
      "\n",
      "批量输入结果:\n",
      "最后一层隐藏状态: torch.Size([2, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# 1. 加载 tokenizer 和 model\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=True)\n",
    "model = AutoModel.from_pretrained(\n",
    "    checkpoint,\n",
    "    output_attentions=True,      # 返回注意力权重（可选）\n",
    "    output_hidden_states=True,   # 返回所有层的隐藏状态（可选）\n",
    ")\n",
    "\n",
    "# 2. 输入文本\n",
    "text = \"Hello, how are you today? I am Grok, built by xAI!\"\n",
    "\n",
    "# 3. 使用 tokenizer 预处理文本\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",         # 返回 PyTorch 张量\n",
    "    max_length=20,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    ")\n",
    "\n",
    "# 4. 将输入传递给模型\n",
    "outputs = model(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    ")\n",
    "\n",
    "# 5. 输出结果\n",
    "print(\"原始文本:\", text)\n",
    "print(\"\\nModel 输出:\")\n",
    "print(\"最后一层隐藏状态 (last_hidden_state):\", outputs.last_hidden_state.shape)\n",
    "print(\"池化输出 (pooler_output):\", outputs.pooler_output.shape)\n",
    "print(\"所有隐藏状态数量:\", len(outputs.hidden_states))\n",
    "print(\"注意力权重数量:\", len(outputs.attentions))\n",
    "\n",
    "# 6. 批量输入示例\n",
    "batch_text = [\"Hello world!\", \"I am Grok.\"]\n",
    "batch_inputs = tokenizer(\n",
    "    batch_text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=10,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "batch_outputs = model(**batch_inputs)  # 使用 ** 解包字典输入\n",
    "print(\"\\n批量输入结果:\")\n",
    "print(\"最后一层隐藏状态:\", batch_outputs.last_hidden_state.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8b4369",
   "metadata": {},
   "source": [
    "bert-base-uncased(110M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82daae73",
   "metadata": {},
   "source": [
    "**输入参数**\n",
    "\n",
    "1. **from_pretrained() 参数**：\n",
    "    - checkpoint: 指定模型名称（如 \"bert-base-uncased\"）。\n",
    "    - cache_dir: 指定缓存路径。\n",
    "    - output_attentions: 是否返回注意力权重。\n",
    "    - output_hidden_states: 是否返回所有层的隐藏状态。\n",
    "2. **model() 参数**：\n",
    "    - input_ids: 来自 tokenizer 的 token ID 张量。\n",
    "    - attention_mask: 注意力掩码，区分有效 token 和填充。\n",
    "    - 可选：token_type_ids（句子对任务）、position_ids 等。\n",
    "\n",
    "**输出内容**\n",
    "\n",
    "1. **last_hidden_state**：\n",
    "    - 形状 [batch_size, sequence_length, hidden_size]。\n",
    "    - 表示最后一层的隐藏状态，每个 token 有一个 768 维向量（对于 BERT-base）。\n",
    "2. **pooler_output**：\n",
    "    - 形状 [batch_size, hidden_size]。\n",
    "    - [CLS] token 的隐藏状态经过池化层（线性 + tanh）后的输出，常用于分类任务。\n",
    "3. **hidden_states**（可选）：\n",
    "    - 一个元组，包含所有层的隐藏状态（包括嵌入层和 12 个 Transformer 层，共 13 个）。\n",
    "    - 每个形状为 [batch_size, sequence_length, hidden_size]。\n",
    "4. **attentions**（可选）：\n",
    "    - 一个元组，包含每层的注意力权重（12 层）。\n",
    "    - 每个形状为 [batch_size, num_heads, sequence_length, hidden_size]，num_heads=12。\n",
    "\n",
    "开启 output_hidden_states 或 output_attentions 会**显著增加内存使用**。\n",
    "\n",
    "**批量输入**\n",
    "\n",
    "- 输入多个句子时，padding=True 确保长度对齐。\n",
    "- 输出张量的 batch_size 变为输入样本数（这里是 2）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916eccb5",
   "metadata": {},
   "source": [
    "**保存模型**：\n",
    "\n",
    "```python\n",
    "model.save_pretrained(\"./my_model\")\n",
    "```\n",
    "**GPU 支持**\n",
    "\n",
    "- 使用 .to(\"cuda\") 将模型和输入移动到 GPU。\n",
    "- 确保 PyTorch 和模型输入都在同一设备上。\n",
    "\n",
    "**冻结参数**：python\n",
    "\n",
    "```python\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False  # 不再更新\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8588ac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handonML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
