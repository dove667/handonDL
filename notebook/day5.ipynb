{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d92d74",
   "metadata": {},
   "source": [
    "# Day 5: LSTM 在 IMDB 情感分类上的实现\n",
    "\n",
    "本讲义演示如何使用 LSTM（长短期记忆）网络在 IMDB 影评数据集上进行**二分类**任务（正面/负面情感）。\n",
    "\n",
    "**学习目标：**\n",
    "- 理解 LSTM 的工作原理与应用场景\n",
    "- 掌握 Hugging Face `datasets` 库用于 NLP 数据加载与预处理\n",
    "- 实现 Embedding + LSTM + Dense 的端到端文本分类流程\n",
    "- 观察并分析训练过程中的过拟合现象\n",
    "\n",
    "**关键概念：**\n",
    "- **Embedding 层**：将词 ID 映射为稠密向量表示\n",
    "- **LSTM 层**：捕捉序列中的长期依赖关系\n",
    "- **动态填充**：根据批次中最长序列进行填充，提高效率\n",
    "- **BCEWithLogitsLoss**：结合 Sigmoid 激活与二分类交叉熵，数值稳定性更好"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pytorch_intro",
   "metadata": {},
   "source": [
    "## Hugging Face `datasets` 库简介\n",
    "\n",
    "`datasets` 是 Hugging Face 生态中的**高效数据加载与处理工具**，专为大规模数据集设计。相比传统方法，它具有以下优势：\n",
    "\n",
    "**核心特性：**\n",
    "1. **高效存储**：基于 Apache Arrow，支持内存映射，即使数据集超过内存也能使用\n",
    "2. **丰富 API**：支持 map、filter、select 等高阶操作，用法简洁\n",
    "3. **自动下载**：自动从 Hugging Face Hub 下载并缓存数据集\n",
    "4. **流式处理**：支持边下载边处理（对超大数据集有用）\n",
    "\n",
    "**使用流程：**\n",
    "1. `load_dataset()`：加载数据集\n",
    "2. 定义预处理函数并用 `map()` 应用\n",
    "3. `set_format()`：转换为 PyTorch 张量\n",
    "4. 与 DataLoader 集成进行批处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b4137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e356f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "max_features = 10000  # 词汇表大小\n",
    "maxlen = 200          # 序列最大长度\n",
    "batch_size = 64       # 批次大小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b832df4f",
   "metadata": {},
   "source": [
    "## `datasets` 库的设计哲学\n",
    "\n",
    "关键点：\n",
    "- 数据存储在**磁盘**上（Arrow 格式），需要时才加载到内存\n",
    "- 支持**高效的切片、索引、过滤**等操作\n",
    "- `map()` 支持 `batched=True` 以加速数据处理\n",
    "- 自动处理数据缓存，避免重复处理\n",
    "\n",
    "这种设计使得即使在资源有限的环境中，也能高效处理数十 GB 级别的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb12d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af32f527cb841b5a4a82b7ff6a69123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6c811f32d74e77a7ede40ba4b7b673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4adc19e517704bffbab506b14b1804eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4cbc7808e424524b020961b6f2eb1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cfd03b11cf4fd18bc93354fc61f5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd34faf8e50a435a93315eaafe7b787d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c4e4f85d5f429b8ba64d439fa499f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dba6c7bad8645bf8ebb4024a907129c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daec4a787fbe49de9d23b866d9304657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd06c0b27f404c84a2e8b2870af3ee6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2c2c8747e84b1383f5ba708ec18d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22ad8bbf5974ad7a18e78e376ebc314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad34cb1f8e348ec9f2537f349599e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75be147adedc4c849d825993323b1e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据加载和预处理完成。\n"
     ]
    }
   ],
   "source": [
    "# 加载 IMDB 数据集\n",
    "dataset = load_dataset('imdb')\n",
    "print(f\"数据集分割: {dataset.column_names}\")\n",
    "print(f\"训练集大小: {len(dataset['train'])}\")\n",
    "print(f\"测试集大小: {len(dataset['test'])}\")\n",
    "\n",
    "# 加载 BERT Tokenizer（用于 IMDB 情感分类）\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 定义预处理函数\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    对文本进行分词、填充和截断\n",
    "    \n",
    "    Args:\n",
    "        examples: 字典，包含 'text' 和 'label' 键\n",
    "        \n",
    "    Returns:\n",
    "        字典，包含 'input_ids', 'attention_mask' 等\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=maxlen\n",
    "    )\n",
    "\n",
    "# 应用预处理（使用 batched=True 加速）\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True,\n",
    "    desc=\"Tokenizing dataset\"\n",
    ")\n",
    "\n",
    "# 后处理：重命名列、移除原始文本\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "\n",
    "# 转换为 PyTorch 张量格式\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "print(\"\\n数据加载和预处理完成。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7ca89e",
   "metadata": {},
   "source": [
    "datasets 提供了强大的数据处理 API，你可以用非常简洁的方式对整个数据集或数据集的某个子集进行转换操作。\n",
    "- map() 方法： 这是最核心的数据处理方法！你可以定义一个函数，然后用 dataset.map(your_function) 将这个函数应用到数据集的每个样本或每个批次上。这使得数据预处理和特征工程变得非常方便。\n",
    "- filter() 方法： 根据条件过滤数据集的样本。\n",
    "- remove_columns() / rename_columns()： 移除或重命名数据集的列。\n",
    "- select()： 选取数据集的某个子集。\n",
    "- sort()： 对数据集进行排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9528079f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader 创建完成。\n"
     ]
    }
   ],
   "source": [
    "# 创建 DataLoader\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=batch_size)\n",
    "\n",
    "print(\"DataLoader 创建完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be268be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本批次形状 (input_ids): torch.Size([64, 200])\n",
      "标签批次形状 (labels): torch.Size([64])\n",
      "Attention mask 批次形状: torch.Size([64, 200])\n",
      "Token type ids 批次形状: torch.Size([64, 200])\n"
     ]
    }
   ],
   "source": [
    "# 检查一个批次的数据形状\n",
    "for batch in train_dataloader:\n",
    "    print(f\"文本批次形状 (input_ids): {batch['input_ids'].shape}\")\n",
    "    print(f\"标签批次形状 (labels): {batch['labels'].shape}\")\n",
    "    # Hugging Face tokenizer 还会返回 attention_mask 和 token_type_ids (对于 BERT)\n",
    "    print(f\"Attention mask 批次形状: {batch['attention_mask'].shape}\")\n",
    "    if 'token_type_ids' in batch:\n",
    "        print(f\"Token type ids 批次形状: {batch['token_type_ids'].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pytorch_model",
   "metadata": {},
   "source": [
    "## 构建简单的 LSTM 模型 \n",
    "\n",
    "搭建一个包含 Embedding 层、LSTM 层和 Dense (Linear) 层的RNN模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pytorch_rnn_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    简单 LSTM 二分类模型\n",
    "    \n",
    "    架构：\n",
    "    - Embedding：词 ID -> 稠密向量 [seq_len, embedding_dim]\n",
    "    - LSTM：单层 LSTM 捕捉序列依赖 [seq_len, hidden_dim]\n",
    "    - 输出层：LSTM 最后隐藏状态 -> 线性层 -> 二分类输出\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding 层：将词 ID (0-vocab_size-1) 映射为 embedding_dim 维向量\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM 层：单向，batch_first=True 使输入为 [batch, seq_len, features]\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=1, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 全连接输出层\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            predictions: [batch_size, output_dim]\n",
    "        \"\"\"\n",
    "        # Embedding: [batch, seq_len] -> [batch, seq_len, embedding_dim]\n",
    "        embedded = self.embedding(input_ids)\n",
    "        \n",
    "        # LSTM: [batch, seq_len, embedding_dim] -> output, (hidden, cell)\n",
    "        # output: [batch, seq_len, hidden_dim]，每个时间步的输出\n",
    "        # hidden: [1, batch, hidden_dim]，最后一个时间步的隐藏状态\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # 取 LSTM 最后一个时间步的隐藏状态\n",
    "        hidden = hidden.squeeze(0)  # [batch, hidden_dim]\n",
    "        \n",
    "        # 全连接层进行分类\n",
    "        predictions = self.fc(hidden)  # [batch, output_dim]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eb3121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型构建完成。\n",
      "SimpleLSTMModel(\n",
      "  (embedding): Embedding(30522, 128)\n",
      "  (lstm): LSTM(128, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 设置模型参数\n",
    "# 使用 Hugging Face tokenizer 的词汇表大小\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 128 # 嵌入维度适当提升，兼顾性能和效果\n",
    "hidden_dim = 128    # LSTM 隐藏层维度适当提升，兼顾性能和效果\n",
    "output_dim = 1      # 输出维度 (二分类)\n",
    "\n",
    "# 实例化LSTM模型\n",
    "model = SimpleLSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "print(\"模型构建完成。\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pytorch_training_setup",
   "metadata": {},
   "source": [
    "## 训练设置\n",
    "\n",
    "定义损失函数和优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pytorch_loss_optimizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cpu\n",
      "损失函数和优化器设置完成。\n"
     ]
    }
   ],
   "source": [
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCEWithLogitsLoss()  # 结合 Sigmoid 和交叉熵，数值稳定\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 移动模型和损失函数到设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "print(f\"使用设备: {device}\")\n",
    "print(\"损失函数和优化器设置完成。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pytorch_training_loop",
   "metadata": {},
   "source": [
    "## 辅助函数定义\n",
    "\n",
    "**`binary_accuracy()`**：计算二分类准确率\n",
    "- 将 logits 通过 Sigmoid 转换为概率 [0, 1]\n",
    "- 四舍五入到 0 或 1，与真实标签比较\n",
    "\n",
    "**`train()`**：单个 epoch 的训练循环\n",
    "- 遍历训练集所有 batch\n",
    "- 计算损失、反向传播、更新参数\n",
    "- 返回平均损失和准确率\n",
    "\n",
    "**`evaluate()`**：在验证/测试集上评估\n",
    "- 不计算梯度（`torch.no_grad()`）\n",
    "- 返回损失和准确率，用于监控模型性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pytorch_train_eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    计算二分类准确率\n",
    "    \n",
    "    Args:\n",
    "        preds: [batch_size] logits（原始预测值，未经激活）\n",
    "        y: [batch_size] 二进制标签 (0 或 1)\n",
    "        \n",
    "    Returns:\n",
    "        准确率 (0.0 - 1.0)\n",
    "    \"\"\"\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))  # 通过 Sigmoid 转换为概率，再四舍五入\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    单个 epoch 的训练\n",
    "    \n",
    "    Returns:\n",
    "        (epoch_loss, epoch_acc)\n",
    "    \"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        predictions = model(input_ids).squeeze(1)\n",
    "        loss = criterion(predictions, labels.float())\n",
    "        \n",
    "        # 计算准确率\n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    \"\"\"\n",
    "    在验证/测试集上评估\n",
    "    \n",
    "    Returns:\n",
    "        (epoch_loss, epoch_acc)\n",
    "    \"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            predictions = model(input_ids).squeeze(1)\n",
    "            loss = criterion(predictions, labels.float())\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a9f57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.644 | Train Acc: 62.61%\n",
      "\t Valid. Loss: 0.560 |  Valid. Acc: 73.64%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.433 | Train Acc: 80.95%\n",
      "\t Valid. Loss: 0.417 |  Valid. Acc: 80.87%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.291 | Train Acc: 88.50%\n",
      "\t Valid. Loss: 0.373 |  Valid. Acc: 83.77%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.213 | Train Acc: 92.15%\n",
      "\t Valid. Loss: 0.407 |  Valid. Acc: 84.17%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.148 | Train Acc: 95.11%\n",
      "\t Valid. Loss: 0.432 |  Valid. Acc: 84.06%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.103 | Train Acc: 96.88%\n",
      "\t Valid. Loss: 0.540 |  Valid. Acc: 82.78%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.068 | Train Acc: 98.15%\n",
      "\t Valid. Loss: 0.540 |  Valid. Acc: 83.11%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.050 | Train Acc: 98.66%\n",
      "\t Valid. Loss: 0.696 |  Valid. Acc: 83.07%\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.95%\n",
      "\t Valid. Loss: 0.696 |  Valid. Acc: 83.42%\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.030 | Train Acc: 99.28%\n",
      "\t Valid. Loss: 0.763 |  Valid. Acc: 83.09%\n"
     ]
    }
   ],
   "source": [
    "train_losses, valid_losses, train_accs, valid_accs = [], [], [], []\n",
    "\n",
    "epochs = 10 # 不要继续训练了，过拟合严重\n",
    "best_valid_loss = float('inf')\n",
    "best_epoch = 0\n",
    "print(\"start training...\")\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_dataloader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, test_dataloader, criterion)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_accs.append(valid_acc)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_epoch = epoch\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': valid_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, f'../model/SimpleLSTM/checkpoint_{epoch+1}.pth')\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Valid. Loss: {valid_loss:.3f} |  Valid. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d06ebe",
   "metadata": {},
   "source": [
    "## 训练与过拟合观察\n",
    "\n",
    "**现象**：模型在 epoch 3 后开始过拟合\n",
    "- 训练损失持续下降\n",
    "- 验证损失开始上升\n",
    "\n",
    "**原因**：\n",
    "- 单层 LSTM 容量不足，难以充分学习\n",
    "- 数据集足够大，但模型过度拟合训练细节\n",
    "- 缺乏足够的正则化（Dropout 在单层 LSTM 中效果有限）\n",
    "\n",
    "**可能的改进方向**（暂不实现）：\n",
    "- 添加更多 Dropout 层\n",
    "- 增加 LSTM 层数\n",
    "- 数据增强或加权损失\n",
    "- Early Stopping\n",
    "\n",
    "本实验重点是理解流程，暂不做过多优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8102537e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.373 | Test Acc: 83.77%\n"
     ]
    }
   ],
   "source": [
    "# 加载最优模型并评估\n",
    "checkpoint = torch.load(f'../model/SimpleLSTM/checkpoint_{best_epoch+1}.pth')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d29df6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正面情感概率: 0.9527\n",
      "预测标签: positive\n"
     ]
    }
   ],
   "source": [
    "# 单条文本预测函数\n",
    "def predict_sentiment(model, tokenizer, sentence, device, maxlen=200):\n",
    "    model.eval()\n",
    "    tokens = tokenizer(sentence, padding='max_length', truncation=True, max_length=maxlen, return_tensors='pt')\n",
    "    input_ids = tokens['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "        prob = torch.sigmoid(output.squeeze(1)).item()\n",
    "    return prob\n",
    "\n",
    "# 示例：预测一条影评\n",
    "sample_text = \"This movie was absolutely fantastic! I loved it.\"\n",
    "prob = predict_sentiment(model, tokenizer, sample_text, device)\n",
    "print(f'正面情感概率: {prob:.4f}')\n",
    "print('预测标签:', 'positive' if prob > 0.5 else 'negative')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handonML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
