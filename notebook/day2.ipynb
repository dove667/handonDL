{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Day 2: PyTorch 核心模块详解\n",
                "\n",
                "本 Notebook 将详细介绍 PyTorch 中 `torch.nn` 和 `torch.optim` 模块的核心组件。我们将深入探讨各个模块的参数含义、使用方法以及应用场景。\n",
                "\n",
                "## 目录\n",
                "1. [nn.Linear (全连接层)](#1-nnlinear-全连接层)\n",
                "2. [nn.ConvNd & Pooling (卷积与池化)](#2-nnconvnd--pooling-卷积与池化)\n",
                "3. [Normalization & Regularization (归一化与正则化)](#3-normalization--regularization-归一化与正则化)\n",
                "4. [Containers (容器)](#4-containers-容器)\n",
                "5. [nn.Loss (损失函数)](#5-nnloss-损失函数)\n",
                "6. [nn.Optim (优化器)](#6-nnoptim-优化器)\n",
                "7. [nn.functional (函数式接口)](#7-nnfunctional-函数式接口)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. nn.Linear (全连接层)\n",
                "\n",
                "全连接层（Fully Connected Layer）是最基础的神经网络层，它对输入数据进行线性变换：$y = xW^T + b$。\n",
                "\n",
                "### 核心参数\n",
                "- `in_features`: 输入特征的大小（输入的最后一个维度）。\n",
                "- `out_features`: 输出特征的大小。\n",
                "- `bias`: 是否使用偏置 $b$，默认为 `True`。\n",
                "\n",
                "### 代码示例"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Input shape: torch.Size([2, 10])\n",
                        "Output shape: torch.Size([2, 5])\n",
                        "Weight shape: torch.Size([5, 10])\n",
                        "Bias shape: torch.Size([5])\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "# 定义一个全连接层：输入维度 10 -> 输出维度 5\n",
                "linear = nn.Linear(in_features=10, out_features=5)\n",
                "\n",
                "# 模拟输入数据：Batch Size = 2, Feature Size = 10\n",
                "input_tensor = torch.randn(2, 10)\n",
                "\n",
                "# 前向传播\n",
                "output = linear(input_tensor)\n",
                "\n",
                "print(\"Input shape:\", input_tensor.shape)   # torch.Size([2, 10])\n",
                "print(\"Output shape:\", output.shape)        # torch.Size([2, 5])\n",
                "print(\"Weight shape:\", linear.weight.shape) # torch.Size([5, 10])\n",
                "print(\"Bias shape:\", linear.bias.shape)     # torch.Size([5])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. nn.ConvNd & Pooling (卷积与池化)\n",
                "\n",
                "卷积神经网络（CNN）的核心组件。最常用的是 `nn.Conv2d`（用于图像）。\n",
                "\n",
                "### 2.1 nn.Conv2d (二维卷积)\n",
                "\n",
                "用于处理图像数据（Batch, Channel, Height, Width）。\n",
                "\n",
                "#### 核心参数详解\n",
                "- `in_channels`: 输入图像的通道数（例如 RGB 图像为 3）。\n",
                "- `out_channels`: 卷积产生的通道数（即卷积核的数量）。\n",
                "- `kernel_size`: 卷积核的大小。可以是整数（如 3 表示 3x3）或元组（如 (3, 5)）。\n",
                "- `stride`: 步长。控制卷积核滑动的步距。默认为 1。\n",
                "- `padding`: 填充。在输入周围填充 0 的层数。常用于保持输出尺寸不变（padding = kernel_size // 2）。\n",
                "- `dilation`: 空洞卷积的间距。用于增加感受野。\n",
                "- `groups`: 分组卷积。默认为 1。设为 `in_channels` 时为深度可分离卷积（Depthwise Conv）。\n",
                "\n",
                "#### 输出尺寸公式\n",
                "$$ H_{out} = \\lfloor \\frac{H_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1 \\rfloor $$\n",
                "\n",
                "### 代码示例"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Input shape: torch.Size([1, 3, 32, 32])\n",
                        "Output shape: torch.Size([1, 16, 32, 32])\n"
                    ]
                }
            ],
            "source": [
                "# 定义一个卷积层\n",
                "# 输入通道 3 (RGB), 输出通道 16, 卷积核 3x3, 步长 1, 填充 1\n",
                "conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
                "\n",
                "# 模拟输入数据：Batch=1, Channel=3, Height=32, Width=32\n",
                "input_img = torch.randn(1, 3, 32, 32)\n",
                "\n",
                "output_img = conv(input_img)\n",
                "\n",
                "print(\"Input shape:\", input_img.shape)   # torch.Size([1, 3, 32, 32])\n",
                "print(\"Output shape:\", output_img.shape) # torch.Size([1, 16, 32, 32]) (尺寸不变，通道变16)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Pooling (池化层)\n",
                "\n",
                "用于下采样，减少特征图尺寸，降低计算量。\n",
                "\n",
                "- **`nn.MaxPool2d`**: 最大池化。取窗口内的最大值，保留最显著特征。\n",
                "- **`nn.AvgPool2d`**: 平均池化。取窗口内的平均值，平滑特征。\n",
                "\n",
                "#### 常用参数\n",
                "- `kernel_size`: 池化窗口大小（如 2 表示 2x2）。\n",
                "- `stride`: 步长，默认等于 `kernel_size`（即不重叠）。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Pooled shape: torch.Size([1, 16, 16, 16])\n"
                    ]
                }
            ],
            "source": [
                "pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "\n",
                "# 输入：[1, 16, 32, 32]\n",
                "pooled_output = pool(output_img)\n",
                "\n",
                "print(\"Pooled shape:\", pooled_output.shape) # torch.Size([1, 16, 16, 16]) (尺寸减半)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Normalization & Regularization (归一化与正则化)\n",
                "\n",
                "### 3.1 nn.BatchNorm2d (批归一化)\n",
                "\n",
                "在卷积层之后使用，加速收敛并防止过拟合。它在 Batch 维度上进行归一化。\n",
                "\n",
                "- **`num_features`**: 输入的通道数（`C`）。\n",
                "- **注意**: BN 层在 `train()` 和 `eval()` 模式下行为不同。\n",
                "  - `train()`: 使用当前 Batch 的均值和方差，并更新全局运行均值/方差。\n",
                "  - `eval()`: 使用全局运行均值和方差。\n",
                "\n",
                "### 3.2 nn.Dropout (随机失活)\n",
                "\n",
                "在训练过程中以概率 `p` 随机将输入张量中的元素置为 0，用于防止过拟合。\n",
                "\n",
                "- **`p`**: 丢弃概率（默认为 0.5）。\n",
                "- **注意**: 仅在 `train()` 模式下生效，在 `eval()` 模式下不做任何操作。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BN output shape: torch.Size([1, 16, 16, 16])\n",
                        "Train mode (some zeros):\n",
                        " tensor([[ 0.0000, -0.0000, -0.2206, -2.7172, -1.2545, -0.0000,  0.1108, -1.7645,\n",
                        "         -1.0419, -0.1367],\n",
                        "        [ 0.0000,  0.8157, -0.0000,  0.0000, -1.3677, -0.0000,  0.0000, -0.0000,\n",
                        "          0.0000,  2.0330],\n",
                        "        [-0.0000, -0.0000, -0.4552, -0.0000,  0.4040,  1.6006, -0.0000, -0.0000,\n",
                        "          0.0000,  0.0000],\n",
                        "        [-1.9678, -0.5675, -0.6396,  0.4800, -0.0000, -0.1160,  4.4152, -0.0000,\n",
                        "          0.0000,  0.0000],\n",
                        "        [ 0.0000,  1.7427, -2.1221,  0.0000,  1.2788, -0.0000,  0.0000, -0.6074,\n",
                        "         -0.0000, -0.0000]])\n",
                        "Eval mode (no zeros):\n",
                        " tensor([[ 0.1692, -0.1750, -0.1103, -1.3586, -0.6272, -0.9493,  0.0554, -0.8822,\n",
                        "         -0.5209, -0.0684],\n",
                        "        [ 0.0232,  0.4078, -1.0458,  0.1475, -0.6839, -0.1480,  0.6321, -0.1751,\n",
                        "          0.1135,  1.0165],\n",
                        "        [-0.4745, -0.5931, -0.2276, -0.4770,  0.2020,  0.8003, -1.5625, -0.3660,\n",
                        "          1.5857,  0.0419],\n",
                        "        [-0.9839, -0.2837, -0.3198,  0.2400, -1.6654, -0.0580,  2.2076, -0.4068,\n",
                        "          0.1854,  0.1212],\n",
                        "        [ 0.9424,  0.8714, -1.0611,  0.5416,  0.6394, -0.0373,  0.7869, -0.3037,\n",
                        "         -1.5537, -0.4349]])\n"
                    ]
                }
            ],
            "source": [
                "# BatchNorm 示例\n",
                "bn = nn.BatchNorm2d(num_features=16)\n",
                "bn_output = bn(pooled_output)\n",
                "print(\"BN output shape:\", bn_output.shape)\n",
                "\n",
                "# Dropout 示例\n",
                "dropout = nn.Dropout(p=0.5)\n",
                "x = torch.randn(5, 10)\n",
                "\n",
                "# 训练模式\n",
                "dropout.train()\n",
                "print(\"Train mode (some zeros):\\n\", dropout(x))\n",
                "\n",
                "# 测试模式\n",
                "dropout.eval()\n",
                "print(\"Eval mode (no zeros):\\n\", dropout(x))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Containers (容器)\n",
                "\n",
                "用于组合多个网络层。\n",
                "\n",
                "### 4.1 nn.Sequential\n",
                "\n",
                "按顺序将模块添加到容器中，数据会按顺序通过这些模块。这是构建简单前馈网络最快的方式。\n",
                "\n",
                "### 4.2 nn.ModuleList\n",
                "\n",
                "像 Python 的 `list` 一样存储 `nn.Module`，但会自动注册参数。**注意**：`ModuleList` 不会自动定义前向传播逻辑，你需要在 `forward` 函数中手动遍历调用。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sequential(\n",
                        "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
                        "  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                        "  (2): ReLU()\n",
                        "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
                        ")\n"
                    ]
                }
            ],
            "source": [
                "# 使用 Sequential 快速搭建一个小网络\n",
                "model = nn.Sequential(\n",
                "    nn.Conv2d(3, 16, 3, 1, 1),\n",
                "    nn.BatchNorm2d(16),\n",
                "    nn.ReLU(),\n",
                "    nn.MaxPool2d(2)\n",
                ")\n",
                "\n",
                "print(model)\n",
                "# output = model(input_img)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. nn.Loss (损失函数)\n",
                "\n",
                "损失函数衡量模型输出与真实标签之间的差异。\n",
                "\n",
                "### 5.1 nn.MSELoss (均方误差损失)\n",
                "- **用途**: 回归问题（预测连续值）。\n",
                "- **公式**: $L = \\frac{1}{N} \\sum (y_{pred} - y_{true})^2$\n",
                "\n",
                "### 5.2 nn.CrossEntropyLoss (交叉熵损失)\n",
                "- **用途**: 多分类问题。\n",
                "- **输入**: 模型输出的 Logits（**未**经过 Softmax 的原始分数）和 类别索引（LongTensor）。\n",
                "- **注意**: 该函数内部已经包含了 `LogSoftmax` 和 `NLLLoss`，所以模型末层**不需要**加 Softmax。\n",
                "\n",
                "### 5.3 nn.BCEWithLogitsLoss\n",
                "- **用途**: 二分类问题。\n",
                "- **输入**: Logits。\n",
                "- **注意**: 内部包含了 Sigmoid，比手动 Sigmoid + BCELoss 数值更稳定。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loss: 1.4665141105651855\n"
                    ]
                }
            ],
            "source": [
                "# CrossEntropyLoss 示例\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "\n",
                "# 假设 Batch=2, 3个类别\n",
                "logits = torch.randn(2, 3) # 模型输出\n",
                "targets = torch.tensor([0, 2]) # 真实标签\n",
                "\n",
                "loss = criterion(logits, targets)\n",
                "print(\"Loss:\", loss.item())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. nn.Optim (优化器)\n",
                "\n",
                "优化器用于根据梯度更新模型参数。\n",
                "\n",
                "### 常用优化器\n",
                "- **`optim.SGD`**: 随机梯度下降。常用参数 `lr` (学习率), `momentum` (动量)。\n",
                "- **`optim.Adam`**: 自适应矩估计。通常收敛更快。常用参数 `lr`。\n",
                "\n",
                "### 标准训练步骤\n",
                "1. `optimizer.zero_grad()`: 清空过往梯度。\n",
                "2. `loss.backward()`: 反向传播计算当前梯度。\n",
                "3. `optimizer.step()`: 根据梯度更新参数。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.optim as optim\n",
                "\n",
                "# 定义优化器，传入需要更新的参数\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "\n",
                "# 模拟训练步\n",
                "optimizer.zero_grad() # 1. 清零\n",
                "# loss = criterion(output, target) # 计算loss\n",
                "# loss.backward() # 2. 反向传播\n",
                "optimizer.step() # 3. 更新参数"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. nn.functional (函数式接口)\n",
                "\n",
                "`torch.nn.functional` (通常导入为 `F`) 包含了与 `nn.Module` 对应的无状态函数。\n",
                "\n",
                "### 何时使用？\n",
                "- 如果层**有可学习参数**（如 Linear, Conv2d, BatchNorm），使用 `nn.Module`。\n",
                "- 如果层**没有参数**（如 ReLU, MaxPool, Softmax），既可以用 `nn.Module` 也可以用 `F`。通常在 `forward` 函数中直接调用 `F` 更简洁。\n",
                "\n",
                "### 常用函数\n",
                "- `F.relu(x)`\n",
                "- `F.softmax(x, dim=1)`\n",
                "- `F.interpolate(x, scale_factor=2, mode='bilinear')`: 上采样/调整图像大小。\n",
                "- `F.pad(x, pad)`: 填充操作。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn.functional as F\n",
                "\n",
                "class MyNet(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
                "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
                "\n",
                "    def forward(self, x):\n",
                "        # 有参数的层使用 self.layer\n",
                "        x = self.conv1(x)\n",
                "        # 无参数的操作可以直接使用 F.relu\n",
                "        x = F.relu(x)\n",
                "        return x"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
